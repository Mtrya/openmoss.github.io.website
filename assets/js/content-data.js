(function () {
  window.SPA_DATA = {
    brand: {
      name: 'OpenMOSS',
      tagline: '开放、可信赖的基础模型研究'
    },
    positions: {
      applyUrl: 'https://fudannlp.feishu.cn/share/base/form/shrcn29UYq1MCpTH0GBZh3AWPPg',
      // 职位详情（双语支持）
      details: [
        {
          id: 'phd',
          title: { zh: '博士研究生', en: 'PhD Students' },
          blocks: [
            {
              subtitle: { zh: '招收对象', en: 'Target Candidates' },
              paragraphs: {
                zh: [
                  '我们主要招收有志于从事大模型领域科学研究和落地应用的学生，并希望有如下特点：思维活跃、积极主动、热爱研究或开发、刻苦勤奋、不怕失败。如果只是为了混学位或好找工作，请勿联系！',
                  '2025年，本组主要招生方向为大模型预训练、AI Infra、新架构、多模态融合、智能体、具身智能，有兴趣学生欢迎联系。'
                ],
                en: [
                  'We primarily recruit students committed to research and applications in large language models, with the following qualities: active thinking, proactive attitude, passion for research or development, diligence, and resilience. If you are only seeking a degree or job prospects, please do not apply.',
                  'In 2025, our main research areas include LLM pre-training, AI Infrastructure, novel architectures, multimodal fusion, agents, and embodied intelligence. Interested students are welcome to contact us.'
                ]
              }
            },
            {
              subtitle: { zh: '重要事项', en: 'Important Notes' },
              paragraphs: {
                zh: [
                  '除项目合作推荐实习外，本组不允许学生在研三、博五之前进行任何形式的实习。',
                  '本组研究生都会根据需要安排一定的工程任务。如果不愿意参与工程项目的，也不要选择本组。'
                ],
                en: [
                  'Except for project-related internships, students are not allowed to take any internships before their third year of master\'s or fifth year of PhD studies.',
                  'All graduate students will be assigned engineering tasks as needed. If you are unwilling to participate in engineering projects, please do not choose our group.'
                ]
              }
            },
            {
              subtitle: { zh: '招生说明', en: 'Admission Information' },
              paragraphs: {
                zh: [
                  '（1）硕士名额：视研究经费而定，近三年内每年学硕1名、保研专硕1-2名、考研专硕约10名。日常科研中我们不区分学硕和专硕，并且都有资格转博。',
                  '（2）博士名额：视研究经费而定，近三年内每年博士名额2-3名。本人在上海创智学院招直博生，和复旦联培，名额不限。建议先拿到上海创智学院 offer 再报名复旦夏令营。',
                  '（3）招生途径：夏令营、秋季保研、考研面试均需通过学院初审后联系，我们会安排组内面试。',
                  '（4）本组面试：重视科研潜质、工程能力和团队协作。高质量完成 nlp-beginner 的学生优先。',
                  '（5）提前进组：希望确定来本组的同学提前进组学习，表现优秀可推荐字节、华为等合作单位实习。',
                  '（6）研究生待遇：在学校补贴基础上提供有竞争力的补助，并对专硕给予一定租房补贴。'
                ],
                en: [
                  '(1) Master\'s positions: Subject to research funding, approximately 1 academic master, 1-2 professional masters (recommendation), and ~10 professional masters (entrance exam) per year. We do not distinguish between academic and professional masters in research, and all are eligible for PhD conversion.',
                  '(2) PhD positions: Subject to research funding, 2-3 positions per year. Direct PhD positions are available at Shanghai Innovation Institute in collaboration with Fudan, with no limit on positions. We recommend securing an offer from Shanghai Innovation Institute before applying to Fudan\'s summer camp.',
                  '(3) Admission channels: Summer camp, autumn recommendation, and entrance exam interviews all require passing the school\'s preliminary review before contacting us. We will arrange group interviews.',
                  '(4) Group interview: We value research potential, engineering capabilities, and teamwork. Students who complete nlp-beginner with high quality are prioritized.',
                  '(5) Early joining: Students who are certain about joining our group are encouraged to join early. Outstanding performers can be recommended for internships at partner companies like ByteDance and Huawei.',
                  '(6) Graduate benefits: Competitive stipends are provided in addition to university subsidies, with housing allowances for professional masters.'
                ]
              }
            }
          ]
        },
        {
          id: 'master',
          title: { zh: '硕士研究生', en: 'Master\'s Students' },
          blocks: [
            {
              subtitle: { zh: '说明', en: 'Note' },
              paragraphs: {
                zh: ['硕士研究生的招生要求与说明请参考上方"博士研究生"部分。'],
                en: ['Please refer to the "PhD Students" section above for master\'s admission requirements and information.']
              }
            }
          ]
        },
        {
          id: 'postdoc',
          title: { zh: '博士后研究员', en: 'Postdoctoral Researchers' },
          blocks: [
            {
              subtitle: { zh: '研究方向', en: 'Research Areas' },
              paragraphs: {
                zh: ['AI Infra', '大语言模型预训练', '多模态大模型', '语音大模型', '具身智能'],
                en: ['AI Infrastructure', 'LLM Pre-training', 'Multimodal Models', 'Speech Models', 'Embodied Intelligence']
              }
            },
            {
              subtitle: { zh: '申请人要求', en: 'Requirements' },
              paragraphs: {
                zh: [
                  '博士期间专业方向为计算机、软件、电子、自动化、数学等相关方向。',
                  '具备国内外优秀大学博士学位，毕业不超过 3 年。',
                  '年龄在 35 周岁以下。',
                  '在人工智能领域发表高水平文章或主持/参与实际项目者优先。',
                  '思维活跃、创新能力强，对研发充满热情，责任心强。'
                ],
                en: [
                  'PhD in Computer Science, Software Engineering, Electronics, Automation, Mathematics, or related fields.',
                  'PhD degree from a reputable university, graduated within 3 years.',
                  'Under 35 years of age.',
                  'Preference for candidates with high-quality publications in AI or experience leading/participating in real projects.',
                  'Active thinking, strong innovation capability, passionate about R&D, and highly responsible.'
                ]
              }
            },
            {
              subtitle: { zh: '工资待遇', en: 'Compensation' },
              paragraphs: {
                zh: [
                  '按照复旦大学对博士后的相关规定提供待遇，享受公寓与福利，可申请国家"博新计划""引进计划"及上海、市级超级博士后项目。',
                  '课题组根据个人研究进展给予额外补贴，并提供优越科研条件。'
                ],
                en: [
                  'Compensation follows Fudan University\'s postdoctoral regulations, with apartment and benefits. Eligible to apply for national programs such as "Postdoctoral Innovation Talent Support Program" and Shanghai/municipal super postdoctoral projects.',
                  'Additional stipends based on individual research progress, with excellent research conditions provided by the group.'
                ]
              }
            },
            {
              subtitle: { zh: '申请方式', en: 'How to Apply' },
              paragraphs: {
                zh: [
                  '申请邮件发送至 llm@fudan.edu.cn，主题注明"应聘博士后 - 姓名 - 专业 - 学校"。',
                  '邮件附个人简历并说明感兴趣的研究方向，初审后我们将与您联系。'
                ],
                en: [
                  'Send application email to llm@fudan.edu.cn with subject line "Postdoc Application - Name - Major - University".',
                  'Attach your CV and specify your research interests. We will contact you after preliminary review.'
                ]
              }
            }
          ]
        },
        {
          id: 'visiting',
          title: { zh: '访问学者', en: 'Visiting Scholars' },
          blocks: [
            {
              subtitle: { zh: '说明', en: 'Note' },
              paragraphs: {
                zh: ['访问学者详细内容持续更新中，如有兴趣请邮件咨询。'],
                en: ['Details for visiting scholars are being updated. Please contact us via email if interested.']
              }
            }
          ]
        },
        {
          id: 'engineer',
          title: { zh: '研究工程师', en: 'Research Engineers' },
          blocks: [
            {
              subtitle: { zh: '关于岗位', en: 'About the Position' },
              paragraphs: {
                zh: ['复旦大学自然语言实验室因科研工作需要，长期招聘科研工程助理，待遇面议。'],
                en: ['Fudan NLP Lab is recruiting research engineering assistants for long-term positions. Compensation is negotiable.']
              }
            },
            {
              subtitle: { zh: '招聘说明', en: 'Job Description' },
              paragraphs: {
                zh: [
                  '参与实验室的 LLM 工程开发项目。',
                  '具备良好的工程经验，熟练掌握 Python，熟悉 PyTorch 并有 NLP 项目经验者优先。',
                  '具有专研精神，工作踏实认真。'
                ],
                en: [
                  'Participate in the lab\'s LLM engineering and development projects.',
                  'Good engineering experience required. Proficiency in Python, familiarity with PyTorch, and NLP project experience preferred.',
                  'Dedicated, diligent, and responsible work attitude.'
                ]
              }
            },
            {
              subtitle: { zh: '申请方式', en: 'How to Apply' },
              paragraphs: {
                zh: [
                  '申请邮件发送至 llm@fudan.edu.cn，主题注明"应聘科研工程助理 - 姓名"。',
                  '邮件中附简历，初审后我们将与您联系。'
                ],
                en: [
                  'Send application email to llm@fudan.edu.cn with subject line "Research Engineer Application - Name".',
                  'Attach your CV. We will contact you after preliminary review.'
                ]
              }
            }
          ]
        },
        {
          id: 'intern',
          title: { zh: '实习生', en: 'Interns' },
          blocks: [
            {
              subtitle: { zh: '职位介绍', en: 'Position Description' },
              paragraphs: {
                zh: ['实习生岗位内容将于近期补充，欢迎提前投递意向。'],
                en: ['Internship details will be updated soon. Early applications are welcome.']
              }
            }
          ]
        }
      ]
    },
    webmaster: {
      members: [
        { name: '郑逸宁', role: '网页设计师' },
        { name: '贺心嘉', role: '网页设计师' }
      ]
    },
    footer: {
      contactLinks: [
        { label: 'GitHub', icon: 'fa-brands fa-github', url: 'https://github.com/OpenMOSS' },
        { label: 'Twitter', icon: 'fa-brands fa-x-twitter', url: 'https://x.com/Open_MOSS' },
        { label: 'Email', icon: 'fa-solid fa-envelope', url: 'mailto:llm@fudan.edu.cn' }
      ]
    },
    publications: {
      infra: [
        {
          title: 'SpeechGPT 2.0-preview: A GPT-4o-level Real-Time Spoken Dialogue System',
          authors: 'Hanfu Chen, Ke Chen, Qinyuan Cheng, Mingshu Chen, Ruifan Deng, Liwei Fan, Zhaoye Fei, QingHui Gao, Yitian Gong, Ching Wing Kwok, Kexin Huang, Yaozhou Jiang, Xingyu Lu, Shimin Li, Zhengyuan Lin, Ruixiao Li, Qian Tu, Jin Wang, Yang Wang, Siyin Wang, Zhe Xu, Chenchen Yang, Donghua Yu, Yuqian Yao, Yucheng Yuan, Chufan Yu, Dong Zhang, YiWei Zhao, Yuqian Zhang, Jun Zhan, Xin Zhang, Xingjian Zhao, Chengyang Zhu',
          venue: '',
          year: '2025',
          support: true,
          alphabetical: true,
          links: [{ type: 'GitHub', url: 'https://github.com/OpenMOSS/SpeechGPT-2.0-preview' }]
        },
        {
          title: 'MOSS-TTSD: Zero-Shot Multi-Speaker Dialogue Speech Synthesis',
          authors: 'Cheng Chang, Ke Chen, Mingshu Chen, Qinyuan Cheng, Ruifan Deng, Liwei Fan, Zhaoye Fei, Qinghui Gao, Yitian Gong, Kexin Huang, Botian Jiang, Yaozhou Jiang, Luozhijie Jin, Ruixiao Li, Shimin Li, Zhengyuan Lin, Xipeng Qiu, Qian Tu, Jin Wang, Ruiming Wang, Wenxuan Wang, Yang Wang, Chenchen Yang, Zhe Xu, Yucheng Yuan, Donghua Yu, Jun Zhan, Dong Zhang, Wenbo Zhang, Xin Zhang, Yuqian Zhang, Yiwei Zhao, Xingjian Zhao',
          venue: '',
          year: '2025',
          support: true,
          alphabetical: true,
          links: [{ type: 'GitHub', url: 'https://github.com/OpenMOSS/MOSS-TTSD' }]
        },
        {
          title: 'MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance',
          authors: 'Hanfu Chen, Ke Chen, Mingshu Chen, Qinyuan Cheng, Zhaoye Fei, Qinghui Gao, Yang Gao, Yitian Gong, Xuanjing Huang, Yaozhou Jiang, Luozhijie Jin, Ruixiao Li, Xipeng Qiu, Ruiming Wang, Yang Wang, Yuanfan Xu, Xiaogui Yang, Zhe Xu, Donghua Yu, Wenbo Zhang, Yiyang Zhang, Xingjian Zhao, Yaqian Zhou',
          venue: '',
          year: '2025',
          support: true,
          alphabetical: true,
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2510.00499' }]
        },
        {
          title: 'Towards Economical Inference: Enabling DeepSeek\'s Multi-Head Latent Attention in Any Transformer-based LLMs',
          authors: 'Tao Ji, Bin Guo, Yuanbin Wu, Qipeng Guo, Lixing Shen, Zhan Chen, Xipeng Qiu, Qi Zhang, Tao Gui',
          venue: 'ACL',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2502.14837' }]
        }
      ],
      multimodal: [
        {
          title: 'SpeechGPT 2.0-preview: A GPT-4o-level Real-Time Spoken Dialogue System',
          authors: 'Hanfu Chen, Ke Chen, Qinyuan Cheng, Mingshu Chen, Ruifan Deng, Liwei Fan, Zhaoye Fei, QingHui Gao, Yitian Gong, Ching Wing Kwok, Kexin Huang, Yaozhou Jiang, Xingyu Lu, Shimin Li, Zhengyuan Lin, Ruixiao Li, Qian Tu, Jin Wang, Yang Wang, Siyin Wang, Zhe Xu, Chenchen Yang, Donghua Yu, Yuqian Yao, Yucheng Yuan, Chufan Yu, Dong Zhang, YiWei Zhao, Yuqian Zhang, Jun Zhan, Xin Zhang, Xingjian Zhao, Chengyang Zhu',
          venue: '',
          year: '2025',
          alphabetical: true,
          links: [{ type: 'GitHub', url: 'https://github.com/OpenMOSS/SpeechGPT-2.0-preview' }]
        },
        {
          title: 'MOSS-TTSD: Zero-Shot Multi-Speaker Dialogue Speech Synthesis',
          authors: 'Cheng Chang, Ke Chen, Mingshu Chen, Qinyuan Cheng, Ruifan Deng, Liwei Fan, Zhaoye Fei, Qinghui Gao, Yitian Gong, Kexin Huang, Botian Jiang, Yaozhou Jiang, Luozhijie Jin, Ruixiao Li, Shimin Li, Zhengyuan Lin, Xipeng Qiu, Qian Tu, Jin Wang, Ruiming Wang, Wenxuan Wang, Yang Wang, Chenchen Yang, Zhe Xu, Yucheng Yuan, Donghua Yu, Jun Zhan, Dong Zhang, Wenbo Zhang, Xin Zhang, Yuqian Zhang, Yiwei Zhao, Xingjian Zhao',
          venue: '',
          year: '2025',
          alphabetical: true,
          links: [{ type: 'GitHub', url: 'https://github.com/OpenMOSS/MOSS-TTSD' }]
        },
        {
          title: 'MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance',
          authors: 'Hanfu Chen, Ke Chen, Mingshu Chen, Qinyuan Cheng, Zhaoye Fei, Qinghui Gao, Yang Gao, Yitian Gong, Xuanjing Huang, Yaozhou Jiang, Luozhijie Jin, Ruixiao Li, Xipeng Qiu, Ruiming Wang, Yang Wang, Yuanfan Xu, Xiaogui Yang, Zhe Xu, Donghua Yu, Wenbo Zhang, Yiyang Zhang, Xingjian Zhao, Yaqian Zhou',
          venue: '',
          year: '2025',
          alphabetical: true,
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2510.00499' }]
        },
        {
          title: 'InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems',
          authors: 'Kexin Huang, Qian Tu, Liwei Fan, Chenchen Yang, Dong Zhang, Shimin Li, Zhaoye Fei, Qinyuan Cheng, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.16381' }]
        },
        {
          title: 'XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs',
          authors: 'Yitian Gong, Luozhijie Jin, Ruifan Deng, Dong Zhang, Xin Zhang, Qinyuan Cheng, Zhaoye Fei, Shimin Li, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.23325' }]
        },
        {
          title: 'CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs',
          authors: 'Jinlan Fu, Shenzhen Huangfu, Hao Fei, Xiaoyu Shen, Bryan Hooi, Xipeng Qiu, See-Kiong Ng',
          venue: 'ICLR',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2501.16629' }]
        },
        {
          title: 'Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety Alignment of Large Vision-Language Models',
          authors: 'Siyin Wang, Xingsong Ye, Qinyuan Cheng, Junwen Duan, Shimin Li, Jinlan Fu, Xipeng Qiu, Xuanjing Huang',
          venue: 'NAACL',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2406.15279' }]
        },
        {
          title: 'VisuoThink: Empowering LVLM Reasoning with Multimodal Tree Search',
          authors: 'Yikun Wang, Siyin Wang, Qinyuan Cheng, Zhaoye Fei, Liang Ding, Qipeng Guo, D. Tao, Xipeng Qiu',
          venue: 'ACL',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2504.09130' }]
        },
        {
          title: 'RoboOmni: Proactive Robot Manipulation in Omni-modal Context',
          authors: 'Siyin Wang, Jinlan Fu, Feihong Liu, Xinzhe He, Huangxuan Wu, Junhao Shi, Kexin Huang, Zhaoye Fei, Jingjing Gong, Zuxuan Wu, Yu-Gang Jiang, See-Kiong Ng, Tat-Seng Chua, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2510.23763' }]
        }
      ],
      reasoning: [
        {
          title: 'Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections',
          authors: 'Bo Wang, Qinyuan Cheng, Runyu Peng, Rong Bao, Peiji Li, Qipeng Guo, Linyang Li, Zhiyuan Zeng, Yunhua Zhou, Xipeng Qiu',
          venue: 'NeurIPS',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2507.00018' }]
        },
        {
          title: 'RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization',
          authors: 'Zeng Zhiyuan, Jiashuo Liu, Zhangyue Yin, Ge Zhang, Wenhao Huang, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2511.04285' }]
        },
        {
          title: 'Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective',
          authors: 'Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Bo Wang, Shimin Li, Yunhua Zhou, Qipeng Guo, Xuanjing Huang, Xipeng Qiu',
          venue: '',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2412.14135' }]
        },
        {
          title: 'In-Memory Learning: A Declarative Learning Framework for Large Language Models',
          authors: 'Bo Wang, Tianxiang Sun, Hang Yan, Siyin Wang, Qingyuan Cheng, Xipeng Qiu',
          venue: '',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2403.02757' }]
        },
        {
          title: 'Game-RL: Synthesizing Multimodal Verifiable Game Data to Boost VLMs\' General Reasoning',
          authors: 'Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Zhiheng Xi, Changhao Jiang, Zhangyue Yin, Yining Zheng, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2505.13886' }]
        }
      ],
      embodied: [
        {
          title: 'World Modeling Makes a Better Planner: Dual Preference Optimization for Embodied Task Planning',
          authors: 'Siyin Wang, Zhaoye Fei, Qinyuan Cheng, Shiduo Zhang, Panpan Cai, Jinlan Fu, Xipeng Qiu',
          venue: 'ACL',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2503.10480' }]
        },
        {
          title: 'LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models',
          authors: 'Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, Jinlan Fu, Jingjing Gong, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2510.13626' }]
        },
        {
          title: 'VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks',
          authors: 'Shiduo Zhang, Zhe Xu, Peiju Liu, Xiaopeng Yu, Yuan Li, Qinghui Gao, Zhaoye Fei, Zhangyue Yin, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu',
          venue: 'ICCV',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2412.18194' }]
        },
        {
          title: 'World-aware Planning Narratives Enhance Large Vision-Language Model Planner',
          authors: 'Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, Jingjing Gong, Xipeng Qiu',
          venue: 'NeurIPS',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.21230' }]
        },
        {
          title: 'Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning',
          authors: 'Zhaoye Fei, Li Ji, Siyin Wang, Junhao Shi, Jingjing Gong, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.23127' }]
        }
      ],
      safety: [
        {
          title: 'Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT',
          authors: 'Zhengfu He, Xuyang Ge, Qiong Tang, Tianxiang Sun, Qinyuan Cheng, Xipeng Qiu',
          venue: '',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2402.12201' }]
        },
        {
          title: 'Automatically Identifying Local and Global Circuits with Linear Computation Graphs',
          authors: 'Xuyang Ge, Fukang Zhu, Wentao Shu, Junxuan Wang, Zhengfu He, Xipeng Qiu',
          venue: '',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2405.13868' }]
        },
        {
          title: 'Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures',
          authors: 'Junxuan Wang, Xuyang Ge, Wentao Shu, Qiong Tang, Yunhua Zhou, Zhengfu He, Xipeng Qiu',
          venue: 'ICLR',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2410.06672' }]
        },
        {
          title: 'Llama Scope: Extracting Millions of Features from Llama-3.1-8B with Sparse Autoencoders',
          authors: 'Zhengfu He, Wentao Shu, Xuyang Ge, Lingjie Chen, Junxuan Wang, Yunhua Zhou, Frances Liu, Qipeng Guo, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang, Xipeng Qiu',
          venue: '',
          year: '2024',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2410.20526' }]
        },
        {
          title: 'Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition',
          authors: 'Zhengfu He, Junxuan Wang, Rui Lin, Xuyang Ge, Wentao Shu, Qiong Tang, Junping Zhang, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2504.20938' }]
        },
        {
          title: 'Attention Layers Add Into Low-Dimensional Residual Subspaces',
          authors: 'Junxuan Wang, Xuyang Ge, Wentao Shu, Zhengfu He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2508.16929' }]
        },
        {
          title: 'Evolution of Concepts in Language Model Pre-Training',
          authors: 'Xuyang Ge, Wentao Shu, Jiaxing Wu, Yunhua Zhou, Zhengfu He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2509.17196' }]
        }
      ],
      arch: [
        {
          title: 'DiRL: an Efficient Training Framework for Diffusion Language Models',
          authors: 'Ying Zhu, Jiaxin Wan, Tianyi Liang, Xu Guo, Xiaoran Liu, Zengfeng Huang, Ziwei He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'GitHub', url: 'https://github.com/OpenMOSS/DiRL' }]
        },
        {
          title: 'Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction',
          authors: 'Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2508.02558' }]
        },
        {
          title: 'LONGLLADA: Unlocking Long Context Capabilities in Diffusion LLMs',
          authors: 'Xiaoran Liu, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.14429' }]
        },
        {
          title: 'Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache',
          authors: 'Xiaoran Liu, Siyang He, Qiqi Wang, Ruixiao Li, Yuerong Song, Zhigeng Liu, Linlin Li, Qun Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2506.11886' }]
        },
        {
          title: 'Thus Spake Long-Context Large Language Model',
          authors: 'Xiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang He, Qiqi Wang, Linlin Li, Qun Liu, Yaqian Zhou, Xuanjing Huang, Xipeng Qiu',
          venue: '',
          year: '2025',
          links: [{ type: 'ArXiv', url: 'https://arxiv.org/abs/2502.17129' }]
        }
      ]
    }
  };
})();
